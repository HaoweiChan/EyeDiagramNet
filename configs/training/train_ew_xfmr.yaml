# lightning.pytorch==2.1.2
seed_everything: true
trainer:
  accelerator: gpu
  strategy:
    class_path: lightning.pytorch.strategies.DDPStrategy
    init_args:
      find_unused_parameters: false
  devices: -1
  num_nodes: 8
  precision: 16-mixed
  fast_dev_run: false
  max_epochs: 10000
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.
  val_check_interval: null
  check_val_every_n_epoch: 5
  num_sanity_val_steps: 0
  log_every_n_steps: 1
  enable_checkpointing: null
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: 1.
  gradient_clip_algorithm: norm
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: true
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        save_dir: ./saved
        name: ew_xfmr
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelSummary
      init_args:
        max_depth: 3
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        filename: 'ew-xfmr-epoch{epoch}-mae{hp_metric:.2f}'
        auto_insert_metric_name: false
        monitor: hp_metric
        mode: min
        save_last: true
        enable_version_counter: true

model:
  class_path: ml.modules.trace_ew_module.TraceEWModule
  init_args:
    ckpt_path: ./saved/ew_xfmr/version_0/checkpoints/last.ckpt
    strict: false
    ew_scaler: 50
    ew_threshold: 0.1
    compile_model: true
    use_laplace_on_fit_end: true
    model:
      class_path: ml.models.eye_width_model.EyeWidthRegressor
      init_args:
        num_types: 3 # (S, G, D)
        model_dim: 384
        output_dim: 3
        num_heads: 16
        num_layers: 12
        freq_length: 128
        dropout: 0.2

data:
  class_path: ml.data.dataloaders.TraceSeqEWDataloader
  init_args:
    data_dirs:
      pattern2_cowos_8mi: /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/cowos_8mi
      pattern2_cowos_9mi: /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/cowos_9mi
      # pattern2_emib: /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/emib
      # pattern2_cowos_8mi_0124: /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/cowos_8mi_0124
      # pattern2_cowos_9mi_0124: /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/cowos_9mi_0124
      # pattern2_emib_9mi_0124: /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/emib_9mi_0124
      # pattern2_emib_10mi_0124: /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/emib_10mi_0124
    label_dir: /proj/siaiadm/ew_predictor/res
    batch_size: 60
    test_size: 0.25

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0001
    weight_decay: 0.01

# lr_scheduler:
#   class_path: torch.optim.lr_scheduler.ExponentialLR
#   init_args:
#     gamma: 0.995

lr_scheduler:
  class_path: ml.utils.lr_schedulers.CosineAnnealingWarmRestartsWithWarmupDecay
  init_args:
    T_0: 20
    T_mult: 2
    eta_min: 0.
    warmup_epochs: 10
    decay_factor: 0.5