# lightning.pytorch
seed_everything: true

trainer:
  accelerator: gpu
  devices: -1
  precision: 16-mixed
  max_epochs: 1000
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  log_every_n_steps: 10
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: saved
      name: snp_ssl_improved
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: train_loss
        dirpath: saved/snp_ssl_improved/checkpoints
        filename: snp-encoder-epoch{epoch:02d}-train_loss{train_loss:.5f}
        auto_insert_metric_name: false
        mode: min
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch
    - class_path: ml.callbacks.visualization_callbacks.SParameterVisualizer
      init_args:
        num_samples: 2
    # - class_path: lightning.pytorch.callbacks.EarlyStopping
    #   init_args:
    #     monitor: train_loss
    #     patience: 50
    #     mode: min
    #     verbose: true
    - class_path: lightning.pytorch.callbacks.GradientAccumulationScheduler
      init_args:
        scheduling: {0: 1, 100: 2, 200: 4}

model:
  class_path: ml.modules.snp_ssl_module.SNPSelfSupervisedModule
  init_args:
    encoder:
      class_path: ml.models.snp_model.OptimizedSNPEmbedding
      init_args:
        model_dim: 384
        freq_length: 256
        use_checkpointing: false
        use_mixed_precision: true
        use_tx_rx_tokens: false
    decoder:
      class_path: ml.models.snp_model.PhysicsAwareSNPDecoder
      init_args:
        model_dim: 384
        freq_length: 256
        num_attention_layers: 2
        num_heads: 8
        mlp_hidden_ratio: 4
        dropout_rate: 0.1
        use_frequency_conv: true
        conv_kernel_size: 5
        use_checkpointing: false
        use_mixed_precision: true
        enforce_reciprocity: true
    loss_fn:
      class_path: ml.utils.snp_losses.ImprovedSNPReconstructionLoss
      init_args:
        magnitude_weight: 1.0
        phase_weight: 1.5
        complex_weight: 0.5
        spectral_weight: 0.3
        use_unwrapped_phase: true
        latent_reg_type: l2
        latent_reg_weight: 0.001
        gradient_penalty_weight: 0.1

data:
  class_path: ml.data.snp_dataloader.SNPDataModule
  init_args:
    data_dirs:
      - /proj/siaiadm/AI_training_data/D2D/vertical/snp_for_UCIE_20250121
      - /proj/siaiadm/AI_training_data/D2D/from_enzo
      - /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/cowos_8mi/snp
      - /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/cowos_9mi/snp
      # - /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/emib/snp
      # - /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/cowos_8mi_0124/snp
      # - /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/cowos_9mi_0124/snp
      # - /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/emib_9mi_0124/snp
      # - /proj/siaiadm/AI_training_data/D2D/UCIe_Trace/pattern2/emib_10mi_0124/snp
    file_pattern: "*.s96p"
    batch_size: 32
    num_workers: 4
    pin_memory: true
    cache_mode: lazy  # 'all', 'lazy', or 'none'
    cache_size: 1000  # Number of files to keep in memory for lazy mode
    group_by_shape: true  # Group files by shape to minimize padding
    sample_files_for_shape: 10  # Sample size to determine directory shapes

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1e-4
    betas: [0.9, 0.999]
    weight_decay: 0.01
    eps: 1e-8

lr_scheduler:
  class_path: ml.utils.lr_schedulers.CosineAnnealingWarmRestartsWithWarmupDecay
  init_args:
    T_0: 20
    T_mult: 2
    eta_min: 0.
    warmup_epochs: 10
    decay_factor: 0.5

# Optional: Override for testing with smaller dataset
# ckpt_path: null  # Path to resume from checkpoint 