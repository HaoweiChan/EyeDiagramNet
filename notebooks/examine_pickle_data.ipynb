{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Eye Diagram Training Data Examination\n",
        "\n",
        "This notebook helps examine the pickle files generated by the training data collector.\n",
        "Each pickle file contains simulation data for a specific trace SNP file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import simulation functions for comparison\n",
        "from simulation.engine.sparam_to_ew import snp_eyewidth_simulation\n",
        "from simulation.parameters.bound_param import ParameterSet\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load and Examine Pickle Files\n",
        "\n",
        "First, let's find and load the pickle files from the output directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure the path to your pickle files\n",
        "# Adjust this path based on your actual output directory structure\n",
        "pickle_dir = Path(\"./data/training_data\")  # Update this path as needed\n",
        "\n",
        "# Find all pickle files\n",
        "pickle_files = list(pickle_dir.rglob(\"*.pkl\"))\n",
        "print(f\"Found {len(pickle_files)} pickle files\")\n",
        "\n",
        "# Show first few files\n",
        "for i, pfile in enumerate(pickle_files[:5]):\n",
        "    print(f\"{i+1}. {pfile.relative_to(pickle_dir)}\")\n",
        "\n",
        "if len(pickle_files) > 5:\n",
        "    print(f\"... and {len(pickle_files) - 5} more files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and examine the first pickle file\n",
        "if pickle_files:\n",
        "    sample_file = pickle_files[0]\n",
        "    print(f\"Examining: {sample_file.name}\\n\")\n",
        "    \n",
        "    with open(sample_file, 'rb') as f:\n",
        "        sample_data = pickle.load(f)\n",
        "    \n",
        "    print(\"Data structure:\")\n",
        "    for key, value in sample_data.items():\n",
        "        if isinstance(value, list):\n",
        "            print(f\"  {key}: list with {len(value)} items\")\n",
        "            if len(value) > 0:\n",
        "                print(f\"    First item type: {type(value[0])}\")\n",
        "                if key == 'configs' and len(value) > 0:\n",
        "                    print(f\"    Config length: {len(value[0])} parameters\")\n",
        "                elif key == 'line_ews' and len(value) > 0:\n",
        "                    print(f\"    Line EW shape: {np.array(value[0]).shape}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {type(value)}\")\n",
        "else:\n",
        "    print(\"No pickle files found. Please check the pickle_dir path.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Structure Analysis\n",
        "\n",
        "Let's examine the structure and content of the collected data in detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if pickle_files and sample_data:\n",
        "    print(\"Detailed data examination:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Examine configs\n",
        "    if sample_data['configs']:\n",
        "        config_array = np.array(sample_data['configs'])\n",
        "        print(f\"\\nConfigs:\")\n",
        "        print(f\"  Shape: {config_array.shape}\")\n",
        "        print(f\"  Min values: {config_array.min(axis=0)[:10]}...\")\n",
        "        print(f\"  Max values: {config_array.max(axis=0)[:10]}...\")\n",
        "        print(f\"  Mean values: {config_array.mean(axis=0)[:10]}...\")\n",
        "    \n",
        "    # Examine line eye widths\n",
        "    if sample_data['line_ews']:\n",
        "        line_ews_array = np.array(sample_data['line_ews'])\n",
        "        print(f\"\\nLine Eye Widths:\")\n",
        "        print(f\"  Shape: {line_ews_array.shape}\")\n",
        "        print(f\"  Min: {line_ews_array.min():.3f}\")\n",
        "        print(f\"  Max: {line_ews_array.max():.3f}\")\n",
        "        print(f\"  Mean: {line_ews_array.mean():.3f}\")\n",
        "        print(f\"  Std: {line_ews_array.std():.3f}\")\n",
        "        print(f\"  Closed eyes (EW < 0): {(line_ews_array < 0).sum()} / {line_ews_array.size}\")\n",
        "    \n",
        "    # Examine SNP files\n",
        "    if sample_data['snp_txs']:\n",
        "        unique_tx = set(sample_data['snp_txs'])\n",
        "        unique_rx = set(sample_data['snp_rxs'])\n",
        "        print(f\"\\nSNP Files:\")\n",
        "        print(f\"  Unique TX files: {len(unique_tx)}\")\n",
        "        print(f\"  Unique RX files: {len(unique_rx)}\")\n",
        "        print(f\"  Sample TX: {list(unique_tx)[0] if unique_tx else 'None'}\")\n",
        "        print(f\"  Sample RX: {list(unique_rx)[0] if unique_rx else 'None'}\")\n",
        "    \n",
        "    # Examine directions\n",
        "    if sample_data['directions']:\n",
        "        directions_array = np.array(sample_data['directions'])\n",
        "        print(f\"\\nDirections:\")\n",
        "        print(f\"  Shape: {directions_array.shape}\")\n",
        "        if directions_array.size > 0:\n",
        "            print(f\"  Unique patterns: {len(set(tuple(row) for row in directions_array))}\")\n",
        "            print(f\"  Sample directions: {directions_array[0]}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Summary Statistics Across All Files\n",
        "\n",
        "Let's aggregate data from all pickle files to get overall statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate data from all pickle files\n",
        "all_configs = []\n",
        "all_line_ews = []\n",
        "all_directions = []\n",
        "file_stats = []\n",
        "\n",
        "print(\"Loading all pickle files...\")\n",
        "for pfile in pickle_files:\n",
        "    try:\n",
        "        with open(pfile, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        \n",
        "        n_samples = len(data.get('configs', []))\n",
        "        file_stats.append({\n",
        "            'file': pfile.name,\n",
        "            'samples': n_samples,\n",
        "            'trace_name': pfile.stem\n",
        "        })\n",
        "        \n",
        "        if data.get('configs'):\n",
        "            all_configs.extend(data['configs'])\n",
        "        if data.get('line_ews'):\n",
        "            all_line_ews.extend(data['line_ews'])\n",
        "        if data.get('directions'):\n",
        "            all_directions.extend(data['directions'])\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {pfile.name}: {e}\")\n",
        "\n",
        "print(f\"\\nLoaded data from {len(file_stats)} files\")\n",
        "print(f\"Total samples: {len(all_configs)}\")\n",
        "print(f\"Total eye width measurements: {len(all_line_ews)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary statistics DataFrame\n",
        "summary_df = pd.DataFrame(file_stats)\n",
        "summary_df = summary_df.sort_values('samples', ascending=False)\n",
        "\n",
        "print(\"File-wise sample counts:\")\n",
        "print(summary_df.head(10))\n",
        "print(f\"\\nTotal files: {len(summary_df)}\")\n",
        "print(f\"Total samples: {summary_df['samples'].sum()}\")\n",
        "print(f\"Average samples per file: {summary_df['samples'].mean():.1f}\")\n",
        "print(f\"Min samples: {summary_df['samples'].min()}\")\n",
        "print(f\"Max samples: {summary_df['samples'].max()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Eye Width Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if all_line_ews:\n",
        "    # Convert to numpy array for analysis\n",
        "    line_ews_array = np.array(all_line_ews)\n",
        "    \n",
        "    print(f\"Eye Width Analysis:\")\n",
        "    print(f\"  Total measurements: {line_ews_array.size}\")\n",
        "    print(f\"  Shape: {line_ews_array.shape}\")\n",
        "    print(f\"  Min: {line_ews_array.min():.3f}\")\n",
        "    print(f\"  Max: {line_ews_array.max():.3f}\")\n",
        "    print(f\"  Mean: {line_ews_array.mean():.3f}\")\n",
        "    print(f\"  Median: {np.median(line_ews_array):.3f}\")\n",
        "    print(f\"  Std: {line_ews_array.std():.3f}\")\n",
        "    \n",
        "    # Analyze closed eyes\n",
        "    closed_eyes = line_ews_array < 0\n",
        "    print(f\"  Closed eyes (EW < 0): {closed_eyes.sum()} ({closed_eyes.mean()*100:.1f}%)\")\n",
        "    \n",
        "    # Analyze very wide eyes (might be artifacts)\n",
        "    wide_eyes = line_ews_array > 95\n",
        "    print(f\"  Very wide eyes (EW > 95): {wide_eyes.sum()} ({wide_eyes.mean()*100:.1f}%)\")\n",
        "    \n",
        "    # Plot distributions\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Overall distribution\n",
        "    axes[0,0].hist(line_ews_array.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
        "    axes[0,0].set_title('Overall Eye Width Distribution')\n",
        "    axes[0,0].set_xlabel('Eye Width')\n",
        "    axes[0,0].set_ylabel('Frequency')\n",
        "    axes[0,0].axvline(line_ews_array.mean(), color='red', linestyle='--', label=f'Mean: {line_ews_array.mean():.2f}')\n",
        "    axes[0,0].legend()\n",
        "    \n",
        "    # Distribution excluding closed eyes\n",
        "    open_eyes = line_ews_array[line_ews_array >= 0]\n",
        "    if len(open_eyes) > 0:\n",
        "        axes[0,1].hist(open_eyes.flatten(), bins=50, alpha=0.7, edgecolor='black', color='green')\n",
        "        axes[0,1].set_title('Eye Width Distribution (Open Eyes Only)')\n",
        "        axes[0,1].set_xlabel('Eye Width')\n",
        "        axes[0,1].set_ylabel('Frequency')\n",
        "        axes[0,1].axvline(open_eyes.mean(), color='red', linestyle='--', label=f'Mean: {open_eyes.mean():.2f}')\n",
        "        axes[0,1].legend()\n",
        "    \n",
        "    # Box plot by line (if multiple lines)\n",
        "    if line_ews_array.ndim > 1 and line_ews_array.shape[1] > 1:\n",
        "        line_data = [line_ews_array[:, i] for i in range(line_ews_array.shape[1])]\n",
        "        axes[1,0].boxplot(line_data, labels=[f'Line {i}' for i in range(len(line_data))])\n",
        "        axes[1,0].set_title('Eye Width Distribution by Line')\n",
        "        axes[1,0].set_ylabel('Eye Width')\n",
        "    else:\n",
        "        axes[1,0].text(0.5, 0.5, 'Single line data or 1D array', ha='center', va='center', transform=axes[1,0].transAxes)\n",
        "        axes[1,0].set_title('Eye Width by Line (N/A)')\n",
        "    \n",
        "    # Cumulative distribution\n",
        "    sorted_ews = np.sort(line_ews_array.flatten())\n",
        "    y_vals = np.arange(1, len(sorted_ews) + 1) / len(sorted_ews)\n",
        "    axes[1,1].plot(sorted_ews, y_vals)\n",
        "    axes[1,1].set_title('Cumulative Distribution of Eye Widths')\n",
        "    axes[1,1].set_xlabel('Eye Width')\n",
        "    axes[1,1].set_ylabel('Cumulative Probability')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No eye width data found in pickle files.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Data Quality Checks and Summary\n",
        "\n",
        "Perform basic data quality checks and generate a summary report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Data Quality Checks:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Check for consistent data lengths\n",
        "inconsistent_files = []\n",
        "for pfile in pickle_files:\n",
        "    try:\n",
        "        with open(pfile, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        \n",
        "        lengths = {key: len(val) for key, val in data.items() if isinstance(val, list)}\n",
        "        if len(set(lengths.values())) > 1:\n",
        "            inconsistent_files.append((pfile.name, lengths))\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking {pfile.name}: {e}\")\n",
        "\n",
        "if inconsistent_files:\n",
        "    print(f\"\\n⚠️  Found {len(inconsistent_files)} files with inconsistent data lengths:\")\n",
        "    for fname, lengths in inconsistent_files:\n",
        "        print(f\"  {fname}: {lengths}\")\n",
        "else:\n",
        "    print(\"✅ All files have consistent data lengths\")\n",
        "\n",
        "# Check for missing or corrupted data\n",
        "if all_line_ews:\n",
        "    line_ews_array = np.array(all_line_ews)\n",
        "    nan_count = np.isnan(line_ews_array).sum()\n",
        "    inf_count = np.isinf(line_ews_array).sum()\n",
        "    \n",
        "    print(f\"\\nData integrity:\")\n",
        "    print(f\"  NaN values: {nan_count}\")\n",
        "    print(f\"  Infinite values: {inf_count}\")\n",
        "    print(f\"  Values < -1 (suspicious): {(line_ews_array < -1).sum()}\")\n",
        "    print(f\"  Values > 100 (suspicious): {(line_ews_array > 100).sum()}\")\n",
        "\n",
        "# File size analysis\n",
        "file_sizes = [pfile.stat().st_size / 1024 for pfile in pickle_files]  # KB\n",
        "if file_sizes:\n",
        "    print(f\"\\nFile sizes:\")\n",
        "    print(f\"  Average: {np.mean(file_sizes):.1f} KB\")\n",
        "    print(f\"  Min: {np.min(file_sizes):.1f} KB\")\n",
        "    print(f\"  Max: {np.max(file_sizes):.1f} KB\")\n",
        "    print(f\"  Total: {np.sum(file_sizes):.1f} KB ({np.sum(file_sizes)/1024:.1f} MB)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and save summary report\n",
        "report = []\n",
        "report.append(\"EYE DIAGRAM TRAINING DATA SUMMARY REPORT\")\n",
        "report.append(\"=\" * 50)\n",
        "report.append(f\"Generated: {pd.Timestamp.now()}\")\n",
        "report.append(f\"Data directory: {pickle_dir}\")\n",
        "report.append(\"\")\n",
        "\n",
        "# Dataset overview\n",
        "report.append(\"DATASET OVERVIEW:\")\n",
        "report.append(f\"  Total pickle files: {len(pickle_files)}\")\n",
        "report.append(f\"  Total samples: {len(all_configs)}\")\n",
        "if len(all_configs) > 0:\n",
        "    report.append(f\"  Parameters per sample: {len(all_configs[0])}\")\n",
        "if len(all_line_ews) > 0:\n",
        "    line_ews_array = np.array(all_line_ews)\n",
        "    report.append(f\"  Lines per sample: {line_ews_array.shape[1] if line_ews_array.ndim > 1 else 1}\")\n",
        "\n",
        "report.append(\"\")\n",
        "\n",
        "# Eye width statistics\n",
        "if len(all_line_ews) > 0:\n",
        "    line_ews_array = np.array(all_line_ews)\n",
        "    report.append(\"EYE WIDTH STATISTICS:\")\n",
        "    report.append(f\"  Mean: {line_ews_array.mean():.3f}\")\n",
        "    report.append(f\"  Std: {line_ews_array.std():.3f}\")\n",
        "    report.append(f\"  Min: {line_ews_array.min():.3f}\")\n",
        "    report.append(f\"  Max: {line_ews_array.max():.3f}\")\n",
        "    report.append(f\"  Closed eyes: {(line_ews_array < 0).sum()} ({(line_ews_array < 0).mean()*100:.1f}%)\")\n",
        "    report.append(\"\")\n",
        "\n",
        "# File statistics\n",
        "if len(file_stats) > 0:\n",
        "    summary_df = pd.DataFrame(file_stats)\n",
        "    report.append(\"FILE STATISTICS:\")\n",
        "    report.append(f\"  Average samples per file: {summary_df['samples'].mean():.1f}\")\n",
        "    report.append(f\"  Min samples per file: {summary_df['samples'].min()}\")\n",
        "    report.append(f\"  Max samples per file: {summary_df['samples'].max()}\")\n",
        "    report.append(\"\")\n",
        "\n",
        "# Print and save report\n",
        "report_text = \"\\n\".join(report)\n",
        "print(report_text)\n",
        "\n",
        "# Save to file\n",
        "report_file = Path(\"training_data_summary.txt\")\n",
        "with open(report_file, 'w') as f:\n",
        "    f.write(report_text)\n",
        "    \n",
        "print(f\"\\nReport saved to: {report_file}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Validation: Compare Pickle Data with Fresh Simulation\n",
        "\n",
        "Let's compare the stored eye width results with fresh calculations using the same parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a subset of files for validation (to avoid long computation)\n",
        "validation_files = pickle_files[:min(3, len(pickle_files))]  # Validate first 3 files\n",
        "print(f\"Validating {len(validation_files)} files...\")\n",
        "\n",
        "validation_results = []\n",
        "comparison_data = {\n",
        "    'pickle_ews': [],\n",
        "    'fresh_ews': [],\n",
        "    'differences': [],\n",
        "    'relative_errors': [],\n",
        "    'file_names': [],\n",
        "    'sample_indices': []\n",
        "}\n",
        "\n",
        "for file_idx, pfile in enumerate(validation_files):\n",
        "    print(f\"\\nValidating {pfile.name}...\")\n",
        "    \n",
        "    try:\n",
        "        # Load pickle data\n",
        "        with open(pfile, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        \n",
        "        n_samples = len(data.get('configs', []))\n",
        "        print(f\"  Found {n_samples} samples\")\n",
        "        \n",
        "        # Validate a subset of samples from each file (to manage computation time)\n",
        "        sample_indices = list(range(0, min(5, n_samples), max(1, n_samples//5)))  # At most 5 samples per file\n",
        "        \n",
        "        for i, sample_idx in enumerate(sample_indices):\n",
        "            print(f\"  Validating sample {sample_idx+1}/{n_samples}...\")\n",
        "            \n",
        "            # Extract data for this sample\n",
        "            config_list = data['configs'][sample_idx]\n",
        "            pickle_ew = np.array(data['line_ews'][sample_idx])\n",
        "            directions = np.array(data['directions'][sample_idx]) if data['directions'][sample_idx] else None\n",
        "            \n",
        "            # Get SNP file paths\n",
        "            snp_horiz = Path(data.get('snp_horiz', [''])[sample_idx] if isinstance(data.get('snp_horiz', []), list) else pfile.parent / f\"{pfile.stem}.s4p\")\n",
        "            snp_tx = Path(data['snp_txs'][sample_idx])\n",
        "            snp_rx = Path(data['snp_rxs'][sample_idx])\n",
        "            \n",
        "            # Reconstruct config object (assuming it follows ParameterSet structure)\n",
        "            # Note: This might need adjustment based on your actual ParameterSet implementation\n",
        "            config = ParameterSet.from_list(config_list)\n",
        "            \n",
        "            try:\n",
        "                # Run fresh simulation\n",
        "                result = snp_eyewidth_simulation(\n",
        "                    config=config,\n",
        "                    snp_files=(snp_horiz, snp_tx, snp_rx),\n",
        "                    directions=directions,\n",
        "                    device=\"cuda\"  # or \"cpu\" based on your preference\n",
        "                )\n",
        "                \n",
        "                # Handle return format (might be tuple or just line_ew)\n",
        "                if isinstance(result, tuple):\n",
        "                    fresh_ew, fresh_directions = result\n",
        "                else:\n",
        "                    fresh_ew = result\n",
        "                    fresh_directions = directions\n",
        "                \n",
        "                fresh_ew = np.array(fresh_ew)\n",
        "                \n",
        "                # Calculate differences\n",
        "                diff = fresh_ew - pickle_ew\n",
        "                rel_error = np.abs(diff) / (np.abs(pickle_ew) + 1e-10)  # Avoid division by zero\n",
        "                \n",
        "                # Store comparison data\n",
        "                comparison_data['pickle_ews'].extend(pickle_ew.flatten())\n",
        "                comparison_data['fresh_ews'].extend(fresh_ew.flatten())\n",
        "                comparison_data['differences'].extend(diff.flatten())\n",
        "                comparison_data['relative_errors'].extend(rel_error.flatten())\n",
        "                comparison_data['file_names'].extend([pfile.name] * len(pickle_ew.flatten()))\n",
        "                comparison_data['sample_indices'].extend([sample_idx] * len(pickle_ew.flatten()))\n",
        "                \n",
        "                # Print sample comparison\n",
        "                print(f\"    Pickle EW: {pickle_ew}\")\n",
        "                print(f\"    Fresh EW:  {fresh_ew}\")\n",
        "                print(f\"    Difference: {diff}\")\n",
        "                print(f\"    Max abs diff: {np.abs(diff).max():.6f}\")\n",
        "                print(f\"    Max rel error: {rel_error.max():.6f}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    Error in fresh simulation: {e}\")\n",
        "                continue\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pfile.name}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nValidation completed!\")\n",
        "print(f\"Total comparisons: {len(comparison_data['differences'])}\")\n",
        "\n",
        "if len(comparison_data['differences']) > 0:\n",
        "    differences = np.array(comparison_data['differences'])\n",
        "    rel_errors = np.array(comparison_data['relative_errors'])\n",
        "    \n",
        "    print(f\"\\nOverall Validation Statistics:\")\n",
        "    print(f\"  Mean absolute difference: {np.abs(differences).mean():.6f}\")\n",
        "    print(f\"  Max absolute difference: {np.abs(differences).max():.6f}\")\n",
        "    print(f\"  RMS difference: {np.sqrt((differences**2).mean()):.6f}\")\n",
        "    print(f\"  Mean relative error: {rel_errors.mean():.6f}\")\n",
        "    print(f\"  Max relative error: {rel_errors.max():.6f}\")\n",
        "    print(f\"  Differences > 1e-3: {(np.abs(differences) > 1e-3).sum()}\")\n",
        "    print(f\"  Relative errors > 1%: {(rel_errors > 0.01).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of comparison results\n",
        "if len(comparison_data['differences']) > 0:\n",
        "    pickle_ews = np.array(comparison_data['pickle_ews'])\n",
        "    fresh_ews = np.array(comparison_data['fresh_ews'])\n",
        "    differences = np.array(comparison_data['differences'])\n",
        "    rel_errors = np.array(comparison_data['relative_errors'])\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # 1. Scatter plot: Pickle vs Fresh\n",
        "    axes[0,0].scatter(pickle_ews, fresh_ews, alpha=0.6, s=20)\n",
        "    axes[0,0].plot([pickle_ews.min(), pickle_ews.max()], [pickle_ews.min(), pickle_ews.max()], 'r--', label='Perfect match')\n",
        "    axes[0,0].set_xlabel('Pickle Eye Width')\n",
        "    axes[0,0].set_ylabel('Fresh Eye Width')\n",
        "    axes[0,0].set_title('Pickle vs Fresh Eye Width')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Histogram of differences\n",
        "    axes[0,1].hist(differences, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[0,1].axvline(0, color='red', linestyle='--', label='Zero difference')\n",
        "    axes[0,1].axvline(differences.mean(), color='orange', linestyle='--', label=f'Mean: {differences.mean():.6f}')\n",
        "    axes[0,1].set_xlabel('Difference (Fresh - Pickle)')\n",
        "    axes[0,1].set_ylabel('Frequency')\n",
        "    axes[0,1].set_title('Distribution of Differences')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Histogram of relative errors\n",
        "    axes[0,2].hist(rel_errors, bins=30, alpha=0.7, edgecolor='black', color='green')\n",
        "    axes[0,2].axvline(rel_errors.mean(), color='red', linestyle='--', label=f'Mean: {rel_errors.mean():.6f}')\n",
        "    axes[0,2].set_xlabel('Relative Error')\n",
        "    axes[0,2].set_ylabel('Frequency')\n",
        "    axes[0,2].set_title('Distribution of Relative Errors')\n",
        "    axes[0,2].legend()\n",
        "    axes[0,2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Absolute differences vs eye width magnitude\n",
        "    axes[1,0].scatter(pickle_ews, np.abs(differences), alpha=0.6, s=20)\n",
        "    axes[1,0].set_xlabel('Pickle Eye Width')\n",
        "    axes[1,0].set_ylabel('|Difference|')\n",
        "    axes[1,0].set_title('Absolute Difference vs Eye Width')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Relative errors vs eye width magnitude  \n",
        "    axes[1,1].scatter(pickle_ews, rel_errors, alpha=0.6, s=20, color='green')\n",
        "    axes[1,1].set_xlabel('Pickle Eye Width')\n",
        "    axes[1,1].set_ylabel('Relative Error')\n",
        "    axes[1,1].set_title('Relative Error vs Eye Width')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Box plot of differences by file\n",
        "    file_names = comparison_data['file_names']\n",
        "    unique_files = list(set(file_names))\n",
        "    if len(unique_files) > 1:\n",
        "        diff_by_file = [differences[np.array(file_names) == fname] for fname in unique_files]\n",
        "        axes[1,2].boxplot(diff_by_file, labels=[fname[:10] + '...' if len(fname) > 10 else fname for fname in unique_files])\n",
        "        axes[1,2].set_ylabel('Difference')\n",
        "        axes[1,2].set_title('Differences by File')\n",
        "        axes[1,2].tick_params(axis='x', rotation=45)\n",
        "    else:\n",
        "        axes[1,2].text(0.5, 0.5, 'Only one file\\nvalidated', ha='center', va='center', transform=axes[1,2].transAxes)\n",
        "        axes[1,2].set_title('Differences by File (N/A)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Summary statistics table\n",
        "    summary_stats = pd.DataFrame({\n",
        "        'Metric': ['Mean Abs Diff', 'Max Abs Diff', 'RMS Diff', 'Mean Rel Error', 'Max Rel Error', '# Large Diffs (>1e-3)', '# Large Rel Errors (>1%)'],\n",
        "        'Value': [\n",
        "            f\"{np.abs(differences).mean():.6f}\",\n",
        "            f\"{np.abs(differences).max():.6f}\", \n",
        "            f\"{np.sqrt((differences**2).mean()):.6f}\",\n",
        "            f\"{rel_errors.mean():.6f}\",\n",
        "            f\"{rel_errors.max():.6f}\",\n",
        "            f\"{(np.abs(differences) > 1e-3).sum()} / {len(differences)}\",\n",
        "            f\"{(rel_errors > 0.01).sum()} / {len(rel_errors)}\"\n",
        "        ]\n",
        "    })\n",
        "    \n",
        "    print(\"\\nValidation Summary Statistics:\")\n",
        "    print(summary_stats.to_string(index=False))\n",
        "    \n",
        "else:\n",
        "    print(\"No validation data available for plotting.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Detailed Sample-by-Sample Analysis\n",
        "\n",
        "For deeper investigation, let's examine individual samples that show significant differences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify and analyze samples with large differences\n",
        "if len(comparison_data['differences']) > 0:\n",
        "    differences = np.array(comparison_data['differences'])\n",
        "    rel_errors = np.array(comparison_data['relative_errors'])\n",
        "    pickle_ews = np.array(comparison_data['pickle_ews'])\n",
        "    fresh_ews = np.array(comparison_data['fresh_ews'])\n",
        "    \n",
        "    # Find samples with large absolute differences\n",
        "    large_abs_diff_threshold = np.percentile(np.abs(differences), 95)  # Top 5% of differences\n",
        "    large_abs_diff_mask = np.abs(differences) > large_abs_diff_threshold\n",
        "    \n",
        "    # Find samples with large relative errors (excluding very small eye widths)\n",
        "    significant_ew_mask = np.abs(pickle_ews) > 1.0  # Only consider significant eye widths\n",
        "    large_rel_error_threshold = np.percentile(rel_errors[significant_ew_mask], 95) if significant_ew_mask.any() else 0.1\n",
        "    large_rel_error_mask = (rel_errors > large_rel_error_threshold) & significant_ew_mask\n",
        "    \n",
        "    print(f\"Analysis of samples with large differences:\")\n",
        "    print(f\"  Large absolute difference threshold (95th percentile): {large_abs_diff_threshold:.6f}\")\n",
        "    print(f\"  Samples with large absolute differences: {large_abs_diff_mask.sum()}\")\n",
        "    print(f\"  Large relative error threshold (95th percentile): {large_rel_error_threshold:.6f}\")\n",
        "    print(f\"  Samples with large relative errors: {large_rel_error_mask.sum()}\")\n",
        "    \n",
        "    # Show worst cases\n",
        "    worst_abs_diff_indices = np.argsort(np.abs(differences))[-5:]  # 5 worst absolute differences\n",
        "    worst_rel_error_indices = np.argsort(rel_errors)[-5:]  # 5 worst relative errors\n",
        "    \n",
        "    print(f\"\\nWorst Absolute Differences:\")\n",
        "    for i, idx in enumerate(reversed(worst_abs_diff_indices)):\n",
        "        print(f\"  {i+1}. Pickle: {pickle_ews[idx]:.6f}, Fresh: {fresh_ews[idx]:.6f}, Diff: {differences[idx]:.6f}\")\n",
        "    \n",
        "    print(f\"\\nWorst Relative Errors:\")\n",
        "    for i, idx in enumerate(reversed(worst_rel_error_indices)):\n",
        "        if pickle_ews[idx] != 0:  # Avoid division by zero cases\n",
        "            print(f\"  {i+1}. Pickle: {pickle_ews[idx]:.6f}, Fresh: {fresh_ews[idx]:.6f}, Rel Error: {rel_errors[idx]:.6f}\")\n",
        "    \n",
        "    # Distribution analysis\n",
        "    print(f\"\\nDistribution Analysis:\")\n",
        "    print(f\"  Perfect matches (diff = 0): {(differences == 0).sum()}\")\n",
        "    print(f\"  Very small diffs (|diff| < 1e-6): {(np.abs(differences) < 1e-6).sum()}\")\n",
        "    print(f\"  Small diffs (1e-6 <= |diff| < 1e-3): {((np.abs(differences) >= 1e-6) & (np.abs(differences) < 1e-3)).sum()}\")\n",
        "    print(f\"  Medium diffs (1e-3 <= |diff| < 1e-1): {((np.abs(differences) >= 1e-3) & (np.abs(differences) < 1e-1)).sum()}\")\n",
        "    print(f\"  Large diffs (|diff| >= 1e-1): {(np.abs(differences) >= 1e-1).sum()}\")\n",
        "    \n",
        "    # Create a detailed comparison DataFrame for inspection\n",
        "    detailed_comparison = pd.DataFrame({\n",
        "        'File': comparison_data['file_names'],\n",
        "        'Sample_Index': comparison_data['sample_indices'],\n",
        "        'Pickle_EW': pickle_ews,\n",
        "        'Fresh_EW': fresh_ews,\n",
        "        'Difference': differences,\n",
        "        'Abs_Difference': np.abs(differences),\n",
        "        'Rel_Error': rel_errors\n",
        "    })\n",
        "    \n",
        "    # Sort by absolute difference for easy inspection\n",
        "    detailed_comparison = detailed_comparison.sort_values('Abs_Difference', ascending=False)\n",
        "    \n",
        "    print(f\"\\nTop 10 samples with largest differences:\")\n",
        "    print(detailed_comparison.head(10).to_string(index=False, float_format='%.6f'))\n",
        "    \n",
        "    # Save detailed comparison to file\n",
        "    detailed_comparison.to_csv('validation_comparison_details.csv', index=False, float_format='%.8f')\n",
        "    print(f\"\\nDetailed comparison saved to: validation_comparison_details.csv\")\n",
        "    \n",
        "else:\n",
        "    print(\"No validation data available for detailed analysis.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Final Summary Report\n",
        "\n",
        "Generate a comprehensive summary report including validation results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary report with validation results\n",
        "report = []\n",
        "report.append(\"EYE DIAGRAM TRAINING DATA SUMMARY REPORT\")\n",
        "report.append(\"=\" * 50)\n",
        "report.append(f\"Generated: {pd.Timestamp.now()}\")\n",
        "report.append(f\"Data directory: {pickle_dir}\")\n",
        "report.append(\"\")\n",
        "\n",
        "# Dataset overview\n",
        "report.append(\"DATASET OVERVIEW:\")\n",
        "report.append(f\"  Total pickle files: {len(pickle_files)}\")\n",
        "report.append(f\"  Total samples: {len(all_configs)}\")\n",
        "if len(all_configs) > 0:\n",
        "    report.append(f\"  Parameters per sample: {len(all_configs[0])}\")\n",
        "if len(all_line_ews) > 0:\n",
        "    line_ews_array = np.array(all_line_ews)\n",
        "    report.append(f\"  Lines per sample: {line_ews_array.shape[1] if line_ews_array.ndim > 1 else 1}\")\n",
        "report.append(\"\")\n",
        "\n",
        "# Eye width statistics\n",
        "if len(all_line_ews) > 0:\n",
        "    line_ews_array = np.array(all_line_ews)\n",
        "    report.append(\"EYE WIDTH STATISTICS:\")\n",
        "    report.append(f\"  Mean: {line_ews_array.mean():.3f}\")\n",
        "    report.append(f\"  Std: {line_ews_array.std():.3f}\")\n",
        "    report.append(f\"  Min: {line_ews_array.min():.3f}\")\n",
        "    report.append(f\"  Max: {line_ews_array.max():.3f}\")\n",
        "    report.append(f\"  Closed eyes: {(line_ews_array < 0).sum()} ({(line_ews_array < 0).mean()*100:.1f}%)\")\n",
        "    report.append(\"\")\n",
        "\n",
        "# File statistics\n",
        "if len(file_stats) > 0:\n",
        "    summary_df = pd.DataFrame(file_stats)\n",
        "    report.append(\"FILE STATISTICS:\")\n",
        "    report.append(f\"  Average samples per file: {summary_df['samples'].mean():.1f}\")\n",
        "    report.append(f\"  Min samples per file: {summary_df['samples'].min()}\")\n",
        "    report.append(f\"  Max samples per file: {summary_df['samples'].max()}\")\n",
        "    report.append(\"\")\n",
        "\n",
        "# Validation statistics (if validation was performed)\n",
        "if 'comparison_data' in locals() and len(comparison_data.get('differences', [])) > 0:\n",
        "    differences = np.array(comparison_data['differences'])\n",
        "    rel_errors = np.array(comparison_data['relative_errors'])\n",
        "    report.append(\"VALIDATION RESULTS (Pickle vs Fresh Simulation):\")\n",
        "    report.append(f\"  Files validated: {len([f for f in locals().get('validation_files', [])])}\")\n",
        "    report.append(f\"  Samples compared: {len(differences)}\")\n",
        "    report.append(f\"  Mean absolute difference: {np.abs(differences).mean():.6f}\")\n",
        "    report.append(f\"  Max absolute difference: {np.abs(differences).max():.6f}\")\n",
        "    report.append(f\"  RMS difference: {np.sqrt((differences**2).mean()):.6f}\")\n",
        "    report.append(f\"  Mean relative error: {rel_errors.mean():.6f}\")\n",
        "    report.append(f\"  Max relative error: {rel_errors.max():.6f}\")\n",
        "    report.append(f\"  Perfect matches: {(differences == 0).sum()}\")\n",
        "    report.append(f\"  Large differences (>1e-3): {(np.abs(differences) > 1e-3).sum()}\")\n",
        "    report.append(f\"  Large relative errors (>1%): {(rel_errors > 0.01).sum()}\")\n",
        "    \n",
        "    # Add interpretation\n",
        "    if np.abs(differences).max() < 1e-6:\n",
        "        report.append(\"  INTERPRETATION: Excellent agreement - differences within numerical precision\")\n",
        "    elif np.abs(differences).max() < 1e-3:\n",
        "        report.append(\"  INTERPRETATION: Very good agreement - small numerical differences\")\n",
        "    elif np.abs(differences).max() < 1e-1:\n",
        "        report.append(\"  INTERPRETATION: Good agreement - some differences may need investigation\")\n",
        "    else:\n",
        "        report.append(\"  INTERPRETATION: Significant differences detected - investigate further\")\n",
        "    report.append(\"\")\n",
        "\n",
        "# Print and save report\n",
        "report_text = \"\\n\".join(report)\n",
        "print(report_text)\n",
        "\n",
        "# Save to file\n",
        "report_file = Path(\"training_data_summary.txt\")\n",
        "with open(report_file, 'w') as f:\n",
        "    f.write(report_text)\n",
        "    \n",
        "print(f\"\\nReport saved to: {report_file}\")\n",
        "if 'comparison_data' in locals() and len(comparison_data.get('differences', [])) > 0:\n",
        "    print(f\"Validation details saved to: validation_comparison_details.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
