# Eye Diagram Training Data Examination
#
# This script helps examine the pickle files generated by the training data collector.
# Each pickle file contains simulation data for a specific trace SNP file.

# ------------------------------------------------------------
# Imports and setup
# ------------------------------------------------------------
import pickle
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path
warnings.filterwarnings('ignore')

# Direction utilities
from simulation.io.direction_utils import get_valid_block_sizes

# Set up plotting style
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)


# ------------------------------------------------------------
# Helper utilities
# ------------------------------------------------------------
def estimate_block_size(direction_array: np.ndarray) -> int:
    """Estimate the smallest consecutive run length (block size) in a 0/1 array."""
    arr = np.asarray(direction_array).astype(int).flatten()
    if arr.size == 0:
        return 0
    change_idx = np.flatnonzero(np.diff(arr) != 0)
    starts = np.r_[0, change_idx + 1]
    ends = np.r_[change_idx, arr.size - 1]
    run_lengths = ends - starts + 1
    return int(run_lengths.min())


# ------------------------------------------------------------
# 1. Load and Examine Pickle Files
# ------------------------------------------------------------
# Configure the path to your pickle files
# Adjust this path based on your actual output directory structure
pickle_dir = Path("./data/training_data")  # Update this path as needed

# Find all pickle files
pickle_files = list(pickle_dir.rglob("*.pkl"))
print(f"Found {len(pickle_files)} pickle files")

# Show first few files
for i, pfile in enumerate(pickle_files[:5]):
    print(f"{i+1}. {pfile.relative_to(pickle_dir)}")

if len(pickle_files) > 5:
    print(f"... and {len(pickle_files) - 5} more files")


# Load and examine the first pickle file
if pickle_files:
    sample_file = pickle_files[0]
    print(f"Examining: {sample_file.name}\n")

    with open(sample_file, 'rb') as f:
        sample_data = pickle.load(f)

    print("Data structure:")
    for key, value in sample_data.items():
        if isinstance(value, list):
            print(f"  {key}: list with {len(value)} items")
            if len(value) > 0:
                print(f"    First item type: {type(value[0])}")
                if key == 'configs' and len(value) > 0:
                    print(f"    Config length: {len(value[0])} parameters")
                elif key == 'line_ews' and len(value) > 0:
                    print(f"    Line EW shape: {np.array(value[0]).shape}")
        else:
            print(f"  {key}: {type(value)}")
else:
    print("No pickle files found. Please check the pickle_dir path.")


# ------------------------------------------------------------
# 2. Data Structure Analysis
# ------------------------------------------------------------
if pickle_files and sample_data:
    print("Detailed data examination:")
    print("=" * 50)

    # Examine configs
    if sample_data['configs']:
        config_array = np.array(sample_data['configs'])
        print(f"\nConfigs:")
        print(f"  Shape: {config_array.shape}")
        print(f"  Min values: {config_array.min(axis=0)[:10]}...")
        print(f"  Max values: {config_array.max(axis=0)[:10]}...")
        print(f"  Mean values: {config_array.mean(axis=0)[:10]}...")

    # Examine line eye widths
    if sample_data['line_ews']:
        line_ews_array = np.array(sample_data['line_ews'])
        print(f"\nLine Eye Widths:")
        print(f"  Shape: {line_ews_array.shape}")
        print(f"  Min: {line_ews_array.min():.3f}")
        print(f"  Max: {line_ews_array.max():.3f}")
        print(f"  Mean: {line_ews_array.mean():.3f}")
        print(f"  Std: {line_ews_array.std():.3f}")
        print(f"  Closed eyes (EW < 0): {(line_ews_array < 0).sum()} / {line_ews_array.size}")

    # Examine SNP files
    if sample_data['snp_txs']:
        unique_tx = set(sample_data['snp_txs'])
        unique_rx = set(sample_data['snp_rxs'])
        print(f"\nSNP Files:")
        print(f"  Unique TX files: {len(unique_tx)}")
        print(f"  Unique RX files: {len(unique_rx)}")
        print(f"  Sample TX: {list(unique_tx)[0] if unique_tx else 'None'}")
        print(f"  Sample RX: {list(unique_rx)[0] if unique_rx else 'None'}")

    # Examine directions
    if sample_data['directions']:
        directions_array = np.array(sample_data['directions'])
        print(f"\nDirections:")
        print(f"  Shape: {directions_array.shape}")
        if directions_array.size > 0:
            print(f"  Unique patterns: {len(set(tuple(row) for row in directions_array))}")
            print(f"  Sample directions: {directions_array[0]}")


# ------------------------------------------------------------
# 3. Summary Statistics Across All Files
# ------------------------------------------------------------
# Aggregate data from all pickle files
all_configs = []
all_line_ews = []
all_directions = []
file_stats = []

print("Loading all pickle files...")
for pfile in pickle_files:
    try:
        with open(pfile, 'rb') as f:
            data = pickle.load(f)

        n_samples = len(data.get('configs', []))
        file_stats.append({
            'file': pfile.name,
            'samples': n_samples,
            'trace_name': pfile.stem
        })

        if data.get('configs'):
            all_configs.extend(data['configs'])
        if data.get('line_ews'):
            all_line_ews.extend(data['line_ews'])
        if data.get('directions'):
            all_directions.extend(data['directions'])

    except Exception as e:
        print(f"Error loading {pfile.name}: {e}")

print(f"\nLoaded data from {len(file_stats)} files")
print(f"Total samples: {len(all_configs)}")
print(f"Total eye width measurements: {len(all_line_ews)}")


# Create summary statistics DataFrame
summary_df = pd.DataFrame(file_stats)
summary_df = summary_df.sort_values('samples', ascending=False)

print("File-wise sample counts:")
print(summary_df.head(10))
print(f"\nTotal files: {len(summary_df)}")
print(f"Total samples: {summary_df['samples'].sum()}")
print(f"Average samples per file: {summary_df['samples'].mean():.1f}")
print(f"Min samples: {summary_df['samples'].min()}")
print(f"Max samples: {summary_df['samples'].max()}")


# ------------------------------------------------------------
# 4. Directions Distribution and Block Size Analysis
# ------------------------------------------------------------
if all_directions:
    print("\nDirections Analysis:")
    print("=" * 50)

    per_sample_stats = []
    for dir_arr in all_directions:
        arr = np.asarray(dir_arr).astype(int).flatten()
        n_lines = int(arr.size)
        if n_lines == 0:
            continue
        block_est = estimate_block_size(arr)
        valid_sizes = get_valid_block_sizes(n_lines)
        is_valid = block_est in valid_sizes
        num_zeros = int((arr == 0).sum())
        num_ones = int((arr == 1).sum())
        per_sample_stats.append({
            'n_lines': n_lines,
            'block_size_estimate': block_est,
            'is_valid_block_size': is_valid,
            'zeros': num_zeros,
            'ones': num_ones,
        })

    if per_sample_stats:
        df_dirs = pd.DataFrame(per_sample_stats)
        print(f"Samples with directions: {len(df_dirs)}")
        print(f"Unique n_lines: {sorted(df_dirs['n_lines'].unique().tolist())}")
        print("Estimated block size frequency (top 10):")
        print(df_dirs['block_size_estimate'].value_counts().head(10))
        print(f"\nInvalid block size estimates (should be 0): {(~df_dirs['is_valid_block_size']).sum()}")

        # Plot histogram of estimated block sizes
        plt.figure(figsize=(10, 5))
        df_dirs['block_size_estimate'].plot(kind='hist', bins=20, edgecolor='black')
        plt.title('Histogram of Estimated Block Sizes (min consecutive 0/1)')
        plt.xlabel('Estimated Block Size')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        # Per n_lines breakdown (first 20 rows)
        top_groups = (
            df_dirs.groupby(['n_lines', 'block_size_estimate'])
                  .size()
                  .reset_index(name='count')
                  .sort_values(['n_lines', 'count'], ascending=[True, False])
        )
        print("\nBlock size counts by n_lines (first 20 rows):")
        print(top_groups.head(20).to_string(index=False))

        # Save details
        df_dirs.to_csv('directions_block_sizes.csv', index=False)
        print("Saved detailed directions analysis to: directions_block_sizes.csv")
    else:
        print("No non-empty directions arrays found.")
else:
    print("No directions data found in pickle files.")


# ------------------------------------------------------------
# 5. Eye Width Distribution Analysis
# ------------------------------------------------------------
if all_line_ews:
    # Convert to numpy array for analysis
    line_ews_array = np.array(all_line_ews)

    print(f"Eye Width Analysis:")
    print(f"  Total measurements: {line_ews_array.size}")
    print(f"  Shape: {line_ews_array.shape}")
    print(f"  Min: {line_ews_array.min():.3f}")
    print(f"  Max: {line_ews_array.max():.3f}")
    print(f"  Mean: {line_ews_array.mean():.3f}")
    print(f"  Median: {np.median(line_ews_array):.3f}")
    print(f"  Std: {line_ews_array.std():.3f}")

    # Analyze closed eyes
    closed_eyes = line_ews_array < 0
    print(f"  Closed eyes (EW < 0): {closed_eyes.sum()} ({closed_eyes.mean()*100:.1f}%)")

    # Analyze very wide eyes (might be artifacts)
    wide_eyes = line_ews_array > 95
    print(f"  Very wide eyes (EW > 95): {wide_eyes.sum()} ({wide_eyes.mean()*100:.1f}%)")

    # Plot distributions
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Overall distribution
    axes[0,0].hist(line_ews_array.flatten(), bins=50, alpha=0.7, edgecolor='black')
    axes[0,0].set_title('Overall Eye Width Distribution')
    axes[0,0].set_xlabel('Eye Width')
    axes[0,0].set_ylabel('Frequency')
    axes[0,0].axvline(line_ews_array.mean(), color='red', linestyle='--', label=f'Mean: {line_ews_array.mean():.2f}')
    axes[0,0].legend()

    # Distribution excluding closed eyes
    open_eyes = line_ews_array[line_ews_array >= 0]
    if len(open_eyes) > 0:
        axes[0,1].hist(open_eyes.flatten(), bins=50, alpha=0.7, edgecolor='black', color='green')
        axes[0,1].set_title('Eye Width Distribution (Open Eyes Only)')
        axes[0,1].set_xlabel('Eye Width')
        axes[0,1].set_ylabel('Frequency')
        axes[0,1].axvline(open_eyes.mean(), color='red', linestyle='--', label=f'Mean: {open_eyes.mean():.2f}')
        axes[0,1].legend()

    # Box plot by line (if multiple lines)
    if line_ews_array.ndim > 1 and line_ews_array.shape[1] > 1:
        line_data = [line_ews_array[:, i] for i in range(line_ews_array.shape[1])]
        axes[1,0].boxplot(line_data, labels=[f'Line {i}' for i in range(len(line_data))])
        axes[1,0].set_title('Eye Width Distribution by Line')
        axes[1,0].set_ylabel('Eye Width')
    else:
        axes[1,0].text(0.5, 0.5, 'Single line data or 1D array', ha='center', va='center', transform=axes[1,0].transAxes)
        axes[1,0].set_title('Eye Width by Line (N/A)')

    # Cumulative distribution
    sorted_ews = np.sort(line_ews_array.flatten())
    y_vals = np.arange(1, len(sorted_ews) + 1) / len(sorted_ews)
    axes[1,1].plot(sorted_ews, y_vals)
    axes[1,1].set_title('Cumulative Distribution of Eye Widths')
    axes[1,1].set_xlabel('Eye Width')
    axes[1,1].set_ylabel('Cumulative Probability')
    axes[1,1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()
else:
    print("No eye width data found in pickle files.")


# ------------------------------------------------------------
# 6. Data Quality Checks and Summary
# ------------------------------------------------------------
print("Data Quality Checks:")
print("=" * 30)

# Check for consistent data lengths
inconsistent_files = []
for pfile in pickle_files:
    try:
        with open(pfile, 'rb') as f:
            data = pickle.load(f)

        lengths = {key: len(val) for key, val in data.items() if isinstance(val, list)}
        if len(set(lengths.values())) > 1:
            inconsistent_files.append((pfile.name, lengths))
    except Exception as e:
        print(f"Error checking {pfile.name}: {e}")

if inconsistent_files:
    print(f"\n⚠️  Found {len(inconsistent_files)} files with inconsistent data lengths:")
    for fname, lengths in inconsistent_files:
        print(f"  {fname}: {lengths}")
else:
    print("✅ All files have consistent data lengths")

# Check for missing or corrupted data
if all_line_ews:
    line_ews_array = np.array(all_line_ews)
    nan_count = np.isnan(line_ews_array).sum()
    inf_count = np.isinf(line_ews_array).sum()

    print(f"\nData integrity:")
    print(f"  NaN values: {nan_count}")
    print(f"  Infinite values: {inf_count}")
    print(f"  Values < -1 (suspicious): {(line_ews_array < -1).sum()}")
    print(f"  Values > 100 (suspicious): {(line_ews_array > 100).sum()}")

# File size analysis
file_sizes = [pfile.stat().st_size / 1024 for pfile in pickle_files]  # KB
if file_sizes:
    print(f"\nFile sizes:")
    print(f"  Average: {np.mean(file_sizes):.1f} KB")
    print(f"  Min: {np.min(file_sizes):.1f} KB")
    print(f"  Max: {np.max(file_sizes):.1f} KB")
    print(f"  Total: {np.sum(file_sizes):.1f} KB ({np.sum(file_sizes)/1024:.1f} MB)")


# Generate and save summary report
report = []
report.append("EYE DIAGRAM TRAINING DATA SUMMARY REPORT")
report.append("=" * 50)
report.append(f"Generated: {pd.Timestamp.now()}")
report.append(f"Data directory: {pickle_dir}")
report.append("")

# Dataset overview
report.append("DATASET OVERVIEW:")
report.append(f"  Total pickle files: {len(pickle_files)}")
report.append(f"  Total samples: {len(all_configs)}")
if len(all_configs) > 0:
    report.append(f"  Parameters per sample: {len(all_configs[0])}")
if len(all_line_ews) > 0:
    line_ews_array = np.array(all_line_ews)
    report.append(f"  Lines per sample: {line_ews_array.shape[1] if line_ews_array.ndim > 1 else 1}")

report.append("")

# Eye width statistics
if len(all_line_ews) > 0:
    line_ews_array = np.array(all_line_ews)
    report.append("EYE WIDTH STATISTICS:")
    report.append(f"  Mean: {line_ews_array.mean():.3f}")
    report.append(f"  Std: {line_ews_array.std():.3f}")
    report.append(f"  Min: {line_ews_array.min():.3f}")
    report.append(f"  Max: {line_ews_array.max():.3f}")
    report.append(f"  Closed eyes: {(line_ews_array < 0).sum()} ({(line_ews_array < 0).mean()*100:.1f}%)")
    report.append("")

# File statistics
if len(file_stats) > 0:
    summary_df = pd.DataFrame(file_stats)
    report.append("FILE STATISTICS:")
    report.append(f"  Average samples per file: {summary_df['samples'].mean():.1f}")
    report.append(f"  Min samples per file: {summary_df['samples'].min()}")
    report.append(f"  Max samples per file: {summary_df['samples'].max()}")
    report.append("")

# Directions summary (if computed)
if 'df_dirs' in locals():
    report.append("DIRECTIONS BLOCK SIZE SUMMARY:")
    vc = df_dirs['block_size_estimate'].value_counts()
    top = vc.head(5)
    for bs, cnt in top.items():
        report.append(f"  Block size {int(bs)}: {int(cnt)} samples")
    invalid = int((~df_dirs['is_valid_block_size']).sum())
    report.append(f"  Invalid block size estimates: {invalid}")
    report.append("")

# Print and save report
report_text = "\n".join(report)
print(report_text)

# Save to file
report_file = Path("training_data_summary.txt")
with open(report_file, 'w') as f:
    f.write(report_text)

print(f"\nReport saved to: {report_file}")


