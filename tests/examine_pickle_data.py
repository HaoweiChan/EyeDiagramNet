#!/usr/bin/env python3
"""
Eye Diagram Training Data Examination Script

This script helps examine the pickle files generated by the training data collector.
Each pickle file contains simulation data for a specific trace SNP file.

Usage:
    python examine_pickle_data.py [--pickle_dir PATH] [--validate] [--max_files N] [--max_samples N]
"""

import pickle
import argparse
import warnings
import traceback
import json
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
warnings.filterwarnings('ignore')

# Try to import seaborn, make it optional
try:
    import seaborn as sns
    sns.set_palette("husl")
except ImportError:
    print("Warning: seaborn not available, using default matplotlib styling")
    sns = None

# Try to import simulation functions for comparison
try:
    from simulation.engine.sparam_to_ew import snp_eyewidth_simulation as legacy_snp_eyewidth_simulation
    VALIDATION_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Could not import validation modules: {e}")
    traceback.print_exc()
    print("Validation features will be disabled.")
    VALIDATION_AVAILABLE = False

# Set up plotting style
plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 8)

def reconstruct_config(data, sample_idx):
    """
    Reconstruct configuration from pickle data.
    
    Args:
        data: Pickle data dictionary
        sample_idx: Sample index to reconstruct
        
    Returns:
        SampleResult object with configuration or dict if SampleResult not available
        
    Raises:
        ValueError: If data is in old/unsupported format or sample index is out of range
    """
    # Check for new format using configs list and meta
    if 'configs' in data and 'meta' in data and 'config_keys' in data['meta']:
        configs_list = data['configs']
        config_keys = data['meta']['config_keys']
        
        # Check if sample index is out of range
        if sample_idx >= len(configs_list):
            raise ValueError(
                f"Sample index {sample_idx} out of range. "
                f"File contains {len(configs_list)} samples."
            )
        
        # Get config values for this sample
        config_values = configs_list[sample_idx]
        
        # Combine keys and values into a dictionary
        if len(config_keys) != len(config_values):
            raise ValueError(
                f"Mismatch between config_keys length ({len(config_keys)}) "
                f"and config_values length ({len(config_values)})"
            )
        
        config_dict = dict(zip(config_keys, config_values))
        
        if VALIDATION_AVAILABLE:
            from simulation.parameters.bound_param import SampleResult
            return SampleResult.from_dict(config_dict)
        else:
            return config_dict  # Return dict if SampleResult not available
    
    # Check for legacy format (config_dicts) - kept for backward compatibility
    if 'config_dicts' in data and len(data['config_dicts']) > sample_idx:
        config_dict = data['config_dicts'][sample_idx]
        if VALIDATION_AVAILABLE:
            from simulation.parameters.bound_param import SampleResult
            return SampleResult.from_dict(config_dict)
        else:
            return config_dict  # Return dict if SampleResult not available
    
    raise ValueError(f"No valid config data found for sample {sample_idx}. "
                    f"Expected 'configs' list with 'config_keys' in meta, or 'config_dicts'.")

def main():
    """Main function to run the complete analysis"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Examine pickle data from training data collector')
    parser.add_argument('--pickle_dir', type=str, default='./data/training_data',
                        help='Path to directory containing pickle files')
    parser.add_argument('--validate', action='store_true',
                        help='Run validation comparison with new simulation')
    parser.add_argument('--max_files', type=int, default=3,
                        help='Maximum number of files to validate (default: 3)')
    parser.add_argument('--max_samples', type=int, default=5,
                        help='Maximum number of samples per file to validate (default: 5)')
    args = parser.parse_args()
    
    print("EYE DIAGRAM TRAINING DATA EXAMINATION")
    print("="*60)
    
    # 1. Load and examine pickle files
    print("\n1. LOADING AND EXAMINING PICKLE FILES")
    print("="*40)
    
    pickle_dir = Path(args.pickle_dir)
    print(f"Looking for pickle files in: {pickle_dir}")
    
    pickle_files = list(pickle_dir.rglob("*.pkl"))
    print(f"Found {len(pickle_files)} pickle files")
    
    if not pickle_files:
        print("No pickle files found. Please check the path.")
        return
    
    # Show first few files
    for i, pfile in enumerate(pickle_files[:5]):
        try:
            print(f"{i+1}. {pfile.relative_to(pickle_dir)}")
        except ValueError:
            print(f"{i+1}. {pfile}")
    
    if len(pickle_files) > 5:
        print(f"... and {len(pickle_files) - 5} more files")
    
    # Load and examine the first pickle file
    sample_data = None
    if pickle_files:
        sample_file = pickle_files[0]
        print(f"\nExamining: {sample_file.name}")
        
        try:
            with open(sample_file, 'rb') as f:
                sample_data = pickle.load(f)
            
            print("\nData structure:")
            for key, value in sample_data.items():
                if isinstance(value, list):
                    print(f"  {key}: list with {len(value)} items")
                    if len(value) > 0:
                        print(f"    First item type: {type(value[0])}")
                        if key == 'configs' and len(value) > 0:
                            print(f"    Config length: {len(value[0])} parameters")
                        elif key == 'config_dicts' and len(value) > 0:
                            print(f"    Config dict keys: {list(value[0].keys())}")
                        elif key == 'line_ews' and len(value) > 0:
                            print(f"    Line EW shape: {np.array(value[0]).shape}")
                elif isinstance(value, dict):
                    print(f"  {key}: dict with keys: {list(value.keys())}")
                    if key == 'meta':
                        for meta_key, meta_value in value.items():
                            print(f"    {meta_key}: {meta_value}")
                else:
                    print(f"  {key}: {type(value)}")
        except Exception as e:
            print(f"Error loading sample file: {e}")
            traceback.print_exc()
    
    # 2. Data Structure Analysis
    print("\n\n2. DATA STRUCTURE ANALYSIS")
    print("="*40)
    
    if sample_data:
        print("Detailed data examination:")
        
        # Examine configs
        if sample_data.get('configs'):
            config_array = np.array(sample_data['configs'])
            print(f"\nConfigs:")
            print(f"  Shape: {config_array.shape}")
            print(f"  Min values: {config_array.min(axis=0)[:10]}...")
            print(f"  Max values: {config_array.max(axis=0)[:10]}...")
            print(f"  Mean values: {config_array.mean(axis=0)[:10]}...")
        
        # Examine line eye widths
        if sample_data.get('line_ews'):
            line_ews_array = np.array(sample_data['line_ews'])
            print(f"\nLine Eye Widths:")
            print(f"  Shape: {line_ews_array.shape}")
            print(f"  Min: {line_ews_array.min():.3f}")
            print(f"  Max: {line_ews_array.max():.3f}")
            print(f"  Mean: {line_ews_array.mean():.3f}")
            print(f"  Std: {line_ews_array.std():.3f}")
            print(f"  Closed eyes (EW < 0): {(line_ews_array < 0).sum()} / {line_ews_array.size}")
        
        # Examine SNP files
        if sample_data.get('snp_drvs'):
            unique_drv = set(sample_data['snp_drvs'])
            unique_odt = set(sample_data['snp_odts'])
            print(f"\nSNP Files:")
            print(f"  Unique TX files: {len(unique_drv)}")
            print(f"  Unique RX files: {len(unique_odt)}")
            print(f"  Sample TX: {list(unique_drv)[0] if unique_drv else 'None'}")
            print(f"  Sample RX: {list(unique_odt)[0] if unique_odt else 'None'}")
        
        # Examine directions
        if sample_data.get('directions'):
            directions_array = np.array(sample_data['directions'])
            print(f"\nDirections:")
            print(f"  Shape: {directions_array.shape}")
            if directions_array.size > 0:
                print(f"  Unique patterns: {len(set(tuple(row) for row in directions_array))}")
                print(f"  Sample directions: {directions_array[0]}")
    
    # 3. Summary Statistics Across All Files
    print("\n\n3. SUMMARY STATISTICS ACROSS ALL FILES")
    print("="*40)
    
    all_configs = []
    all_line_ews = []
    all_directions = []
    file_stats = []
    
    print("Loading all pickle files...")
    for pfile in pickle_files:
        try:
            with open(pfile, 'rb') as f:
                data = pickle.load(f)
            
            n_samples = len(data.get('configs', []))
            file_stats.append({
                'file': pfile.name,
                'samples': n_samples,
                'trace_name': pfile.stem
            })
            
            if data.get('configs'):
                all_configs.extend(data['configs'])
            if data.get('line_ews'):
                all_line_ews.extend(data['line_ews'])
            if data.get('directions'):
                all_directions.extend(data['directions'])
                
        except Exception as e:
            print(f"Error loading {pfile.name}: {e}")
            traceback.print_exc()
    
    print(f"\nLoaded data from {len(file_stats)} files")
    print(f"Total samples: {len(all_configs)}")
    print(f"Total eye width measurements: {len(all_line_ews)}")
    
    # Create summary statistics DataFrame
    if file_stats:
        summary_df = pd.DataFrame(file_stats)
        summary_df = summary_df.sort_values('samples', ascending=False)
        
        print("\nFile-wise sample counts:")
        print(summary_df.head(10))
        print(f"\nTotal files: {len(summary_df)}")
        print(f"Total samples: {summary_df['samples'].sum()}")
        print(f"Average samples per file: {summary_df['samples'].mean():.1f}")
        print(f"Min samples: {summary_df['samples'].min()}")
        print(f"Max samples: {summary_df['samples'].max()}")
    
    # 4. Eye Width Distribution Analysis
    print("\n\n4. EYE WIDTH DISTRIBUTION ANALYSIS")
    print("="*40)
    
    if all_line_ews:
        line_ews_array = np.array(all_line_ews)
        
        print(f"Eye Width Analysis:")
        print(f"  Total measurements: {line_ews_array.size}")
        print(f"  Shape: {line_ews_array.shape}")
        print(f"  Min: {line_ews_array.min():.3f}")
        print(f"  Max: {line_ews_array.max():.3f}")
        print(f"  Mean: {line_ews_array.mean():.3f}")
        print(f"  Median: {np.median(line_ews_array):.3f}")
        print(f"  Std: {line_ews_array.std():.3f}")
        
        # Analyze closed eyes
        closed_eyes = line_ews_array < 0
        print(f"  Closed eyes (EW < 0): {closed_eyes.sum()} ({closed_eyes.mean()*100:.1f}%)")
        
        # Analyze very wide eyes
        wide_eyes = line_ews_array > 95
        print(f"  Very wide eyes (EW > 95): {wide_eyes.sum()} ({wide_eyes.mean()*100:.1f}%)")
        
        # Create distribution plots
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # Overall distribution
            axes[0,0].hist(line_ews_array.flatten(), bins=50, alpha=0.7, edgecolor='black')
            axes[0,0].set_title('Overall Eye Width Distribution')
            axes[0,0].set_xlabel('Eye Width')
            axes[0,0].set_ylabel('Frequency')
            axes[0,0].axvline(line_ews_array.mean(), color='red', linestyle='--', 
                             label=f'Mean: {line_ews_array.mean():.2f}')
            axes[0,0].legend()
            
            # Distribution excluding closed eyes
            open_eyes = line_ews_array[line_ews_array >= 0]
            if len(open_eyes) > 0:
                axes[0,1].hist(open_eyes.flatten(), bins=50, alpha=0.7, edgecolor='black', color='green')
                axes[0,1].set_title('Eye Width Distribution (Open Eyes Only)')
                axes[0,1].set_xlabel('Eye Width')
                axes[0,1].set_ylabel('Frequency')
                axes[0,1].axvline(open_eyes.mean(), color='red', linestyle='--', 
                                 label=f'Mean: {open_eyes.mean():.2f}')
                axes[0,1].legend()
            
            # Box plot by line (if multiple lines)
            if line_ews_array.ndim > 1 and line_ews_array.shape[1] > 1:
                line_data = [line_ews_array[:, i] for i in range(line_ews_array.shape[1])]
                axes[1,0].boxplot(line_data, labels=[f'Line {i}' for i in range(len(line_data))])
                axes[1,0].set_title('Eye Width Distribution by Line')
                axes[1,0].set_ylabel('Eye Width')
            else:
                axes[1,0].text(0.5, 0.5, 'Single line data or 1D array', 
                              ha='center', va='center', transform=axes[1,0].transAxes)
                axes[1,0].set_title('Eye Width by Line (N/A)')
            
            # Cumulative distribution
            sorted_ews = np.sort(line_ews_array.flatten())
            y_vals = np.arange(1, len(sorted_ews) + 1) / len(sorted_ews)
            axes[1,1].plot(sorted_ews, y_vals)
            axes[1,1].set_title('Cumulative Distribution of Eye Widths')
            axes[1,1].set_xlabel('Eye Width')
            axes[1,1].set_ylabel('Cumulative Probability')
            axes[1,1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig('eye_width_distributions.png', dpi=300, bbox_inches='tight')
            print("\nDistribution plots saved as: eye_width_distributions.png")
            plt.show()
        except Exception as e:
            print(f"Error creating plots: {e}")
            traceback.print_exc()
    else:
        print("No eye width data found in pickle files.")
    
    # 5. Data Quality Checks
    print("\n\n5. DATA QUALITY CHECKS")
    print("="*40)
    
    # Check for consistent data lengths
    inconsistent_files = []
    for pfile in pickle_files:
        try:
            with open(pfile, 'rb') as f:
                data = pickle.load(f)
            
            lengths = {key: len(val) for key, val in data.items() if isinstance(val, list)}
            if len(set(lengths.values())) > 1:
                inconsistent_files.append((pfile.name, lengths))
        except Exception as e:
            print(f"Error checking {pfile.name}: {e}")
            traceback.print_exc()
    
    if inconsistent_files:
        print(f"\n⚠️  Found {len(inconsistent_files)} files with inconsistent data lengths:")
        for fname, lengths in inconsistent_files:
            print(f"  {fname}: {lengths}")
    else:
        print("✅ All files have consistent data lengths")
    
    # Check for missing or corrupted data
    if all_line_ews:
        line_ews_array = np.array(all_line_ews)
        nan_count = np.isnan(line_ews_array).sum()
        inf_count = np.isinf(line_ews_array).sum()
        
        print(f"\nData integrity:")
        print(f"  NaN values: {nan_count}")
        print(f"  Infinite values: {inf_count}")
        print(f"  Values < -1 (suspicious): {(line_ews_array < -1).sum()}")
        print(f"  Values > 100 (suspicious): {(line_ews_array > 100).sum()}")
    
    # File size analysis
    file_sizes = [pfile.stat().st_size / 1024 for pfile in pickle_files]  # KB
    if file_sizes:
        print(f"\nFile sizes:")
        print(f"  Average: {np.mean(file_sizes):.1f} KB")
        print(f"  Min: {np.min(file_sizes):.1f} KB")
        print(f"  Max: {np.max(file_sizes):.1f} KB")
        print(f"  Total: {np.sum(file_sizes):.1f} KB ({np.sum(file_sizes)/1024:.1f} MB)")
    
    # 6. Validation (if requested and available)
    comparison_data = {}
    detailed_validation_results = []  # Store detailed results for report
    if args.validate and VALIDATION_AVAILABLE:
        print("\n\n6. VALIDATION: COMPARE PICKLE DATA WITH SIMULATED DATA")
        print("="*40)
        
        # Select random files for validation instead of first few files
        num_files_to_validate = min(args.max_files, len(pickle_files))
        validation_files = random.sample(pickle_files, num_files_to_validate)
        print(f"Validating {len(validation_files)} randomly selected files...")
        print("Selected files:")
        for i, vfile in enumerate(validation_files):
            print(f"  {i+1}. {vfile.name}")
        
        comparison_data = {
            'pickle_ews': [],
            'simulated_ews': [],
            'differences': [],
            'relative_errors': [],
            'file_names': [],
            'sample_indices': []
        }
        
        for file_idx, pfile in enumerate(validation_files):
            print(f"\nValidating {pfile.name}...")
            
            try:
                with open(pfile, 'rb') as f:
                    data = pickle.load(f)
                
                snp_horiz = Path(data['meta']['snp_horiz'])
                n_samples = len(data.get('configs', []))
                print(f"  Found {n_samples} samples")
                
                # Validate a subset of samples from each file
                sample_indices = list(range(0, min(args.max_samples, n_samples), 
                                          max(1, n_samples//args.max_samples)))
                
                for i, sample_idx in enumerate(sample_indices):
                    print(f"  Validating sample {sample_idx+1}/{n_samples}...")
                    
                    # Extract data for this sample
                    pickle_ew = np.array(data['line_ews'][sample_idx])
                    directions = np.array(data['directions'][sample_idx]) if data['directions'][sample_idx] else None
                    
                    # Get SNP file paths - use n_ports from meta to construct correct filename
                    snp_drv = Path(data['snp_drvs'][sample_idx])
                    snp_odt = Path(data['snp_odts'][sample_idx])
                    
                    try:
                        # Reconstruct config object - handle both old and new formats
                        config = reconstruct_config(data, sample_idx)
                        
                        # Run simulation
                        result = legacy_snp_eyewidth_simulation(
                            config,
                            (snp_horiz, snp_drv, snp_odt),
                            directions,
                            device="cpu"
                        )
                        
                        # Handle return format
                        if isinstance(result, tuple):
                            simulated_ew, simulated_directions = result
                        else:
                            simulated_ew = result
                        
                        simulated_ew = np.array(simulated_ew)
                        
                        # Calculate differences
                        diff = simulated_ew - pickle_ew
                        rel_error = np.abs(diff) / (np.abs(pickle_ew) + 1e-10)
                        
                        # Store detailed validation result
                        config_dict = config if isinstance(config, dict) else config.to_dict()
                        validation_result = {
                            'file_name': pfile.name,
                            'sample_index': sample_idx,
                            'config': config_dict,
                            'snp_drv': str(snp_drv),
                            'snp_odt': str(snp_odt),
                            'directions': directions.tolist() if directions is not None else None,
                            'pickle_ew': pickle_ew.tolist(),
                            'simulated_ew': simulated_ew.tolist(),
                            'differences': diff.tolist(),
                            'abs_differences': np.abs(diff).tolist(),
                            'relative_errors': rel_error.tolist(),
                            'max_abs_diff': np.abs(diff).max(),
                            'max_rel_error': rel_error.max(),
                            'perfect_match': np.allclose(pickle_ew, simulated_ew, atol=1e-10)
                        }
                        detailed_validation_results.append(validation_result)
                        
                        # Store comparison data
                        comparison_data['pickle_ews'].extend(pickle_ew.flatten())
                        comparison_data['simulated_ews'].extend(simulated_ew.flatten())
                        comparison_data['differences'].extend(diff.flatten())
                        comparison_data['relative_errors'].extend(rel_error.flatten())
                        comparison_data['file_names'].extend([pfile.name] * len(pickle_ew.flatten()))
                        comparison_data['sample_indices'].extend([sample_idx] * len(pickle_ew.flatten()))
                        
                        # Print sample comparison
                        print("    Config:")
                        config_dict = config if isinstance(config, dict) else config.to_dict()
                        for k, v in config_dict.items():
                            print(f"      {k}: {v:.1e}")
                        print(f"    SNP horiz: {snp_horiz}")
                        print(f"    SNP TX: {snp_drv}")
                        print(f"    SNP RX: {snp_odt}")
                        print(f"    Directions: {', '.join(map(str, directions.tolist())) if directions is not None else 'None'}")
                        print(f"    Pickle EW: {pickle_ew}")
                        print(f"    Simulated EW:  {simulated_ew}")
                        print(f"    Difference: {diff}")
                        print(f"    Max abs diff: {np.abs(diff).max():.1f}")
                        print(f"    Max rel error: {rel_error.max():.1f}")
                        
                    except Exception as e:
                        print(f"    Error in simulation: {e}")
                        traceback.print_exc()
                        
                        # Store failed validation result
                        try:
                            config = reconstruct_config(data, sample_idx)
                            config_dict = config if isinstance(config, dict) else config.to_dict()
                        except:
                            config_dict = {"error": "Could not reconstruct config"}
                        
                        validation_result = {
                            'file_name': pfile.name,
                            'sample_index': sample_idx,
                            'config': config_dict,
                            'snp_drv': str(snp_drv),
                            'snp_odt': str(snp_odt),
                            'directions': directions.tolist() if directions is not None else None,
                            'pickle_ew': pickle_ew.tolist(),
                            'simulated_ew': None,
                            'differences': None,
                            'abs_differences': None,
                            'relative_errors': None,
                            'max_abs_diff': None,
                            'max_rel_error': None,
                            'perfect_match': False,
                            'error': str(e)
                        }
                        detailed_validation_results.append(validation_result)
                        continue
                        
            except Exception as e:
                print(f"Error processing {pfile.name}: {e}")
                traceback.print_exc()
                continue
        
        print(f"\nValidation completed!")
        print(f"Total comparisons: {len(comparison_data['differences'])}")
        
        if len(comparison_data['differences']) > 0:
            differences = np.array(comparison_data['differences'])
            rel_errors = np.array(comparison_data['relative_errors'])
            
            print(f"\nOverall Validation Statistics:")
            print(f"  Mean absolute difference: {np.abs(differences).mean():.6f}")
            print(f"  Max absolute difference: {np.abs(differences).max():.6f}")
            print(f"  RMS difference: {np.sqrt((differences**2).mean()):.6f}")
            print(f"  Mean relative error: {rel_errors.mean():.6f}")
            print(f"  Max relative error: {rel_errors.max():.6f}")
            print(f"  Differences > 1e-3: {(np.abs(differences) > 1e-3).sum()}")
            print(f"  Relative errors > 1%: {(rel_errors > 0.01).sum()}")
            
            # Create validation plots
            try:
                pickle_ews = np.array(comparison_data['pickle_ews'])
                simulated_ews = np.array(comparison_data['simulated_ews'])
                
                fig, axes = plt.subplots(2, 3, figsize=(18, 12))
                
                # Scatter plot: Pickle vs Simulated
                axes[0,0].scatter(pickle_ews, simulated_ews, alpha=0.6, s=20)
                axes[0,0].plot([pickle_ews.min(), pickle_ews.max()], 
                              [pickle_ews.min(), pickle_ews.max()], 'r--', label='Perfect match')
                axes[0,0].set_xlabel('Pickle Eye Width')
                axes[0,0].set_ylabel('Simulated Eye Width')
                axes[0,0].set_title('Pickle vs Simulated Eye Width')
                axes[0,0].legend()
                axes[0,0].grid(True, alpha=0.3)
                
                # Histogram of differences
                axes[0,1].hist(differences, bins=30, alpha=0.7, edgecolor='black')
                axes[0,1].axvline(0, color='red', linestyle='--', label='Zero difference')
                axes[0,1].axvline(differences.mean(), color='orange', linestyle='--', 
                                 label=f'Mean: {differences.mean():.6f}')
                axes[0,1].set_xlabel('Difference (Simulated - Pickle)')
                axes[0,1].set_ylabel('Frequency')
                axes[0,1].set_title('Distribution of Differences')
                axes[0,1].legend()
                axes[0,1].grid(True, alpha=0.3)
                
                # Histogram of relative errors
                axes[0,2].hist(rel_errors, bins=30, alpha=0.7, edgecolor='black', color='green')
                axes[0,2].axvline(rel_errors.mean(), color='red', linestyle='--', 
                                 label=f'Mean: {rel_errors.mean():.6f}')
                axes[0,2].set_xlabel('Relative Error')
                axes[0,2].set_ylabel('Frequency')
                axes[0,2].set_title('Distribution of Relative Errors')
                axes[0,2].legend()
                axes[0,2].grid(True, alpha=0.3)
                
                # Absolute differences vs eye width magnitude
                axes[1,0].scatter(pickle_ews, np.abs(differences), alpha=0.6, s=20)
                axes[1,0].set_xlabel('Pickle Eye Width')
                axes[1,0].set_ylabel('|Difference|')
                axes[1,0].set_title('Absolute Difference vs Eye Width')
                axes[1,0].grid(True, alpha=0.3)
                
                # Relative errors vs eye width magnitude  
                axes[1,1].scatter(pickle_ews, rel_errors, alpha=0.6, s=20, color='green')
                axes[1,1].set_xlabel('Pickle Eye Width')
                axes[1,1].set_ylabel('Relative Error')
                axes[1,1].set_title('Relative Error vs Eye Width')
                axes[1,1].grid(True, alpha=0.3)
                
                # Box plot of differences by file
                file_names = comparison_data['file_names']
                unique_files = list(set(file_names))
                if len(unique_files) > 1:
                    diff_by_file = [differences[np.array(file_names) == fname] for fname in unique_files]
                    axes[1,2].boxplot(diff_by_file, 
                                     labels=[fname[:10] + '...' if len(fname) > 10 else fname 
                                            for fname in unique_files])
                    axes[1,2].set_ylabel('Difference')
                    axes[1,2].set_title('Differences by File')
                    axes[1,2].tick_params(axis='x', rotation=45)
                else:
                    axes[1,2].text(0.5, 0.5, 'Only one file\nvalidated', 
                                  ha='center', va='center', transform=axes[1,2].transAxes)
                    axes[1,2].set_title('Differences by File (N/A)')
                
                plt.tight_layout()
                plt.savefig('validation_comparison.png', dpi=300, bbox_inches='tight')
                print("\nValidation plots saved as: validation_comparison.png")
                plt.show()
                
                # Save detailed comparison
                detailed_comparison = pd.DataFrame(detailed_validation_results)
                detailed_comparison.to_csv('validation_comparison_details.csv', index=False, float_format='%.8f')
                print("Detailed comparison saved to: validation_comparison_details.csv")
                
            except Exception as e:
                print(f"Error creating validation plots: {e}")
                traceback.print_exc()
    
    elif args.validate and not VALIDATION_AVAILABLE:
        print("\nValidation requested but not available due to missing imports.")
    
    # 7. Generate Final Report
    print("\n\n7. FINAL SUMMARY REPORT")
    print("="*40)
    
    report = []
    report.append("EYE DIAGRAM TRAINING DATA SUMMARY REPORT")
    report.append("=" * 50)
    report.append(f"Generated: {pd.Timestamp.now()}")
    report.append(f"Data directory: {pickle_dir}")
    report.append("")
    
    # Dataset overview
    report.append("DATASET OVERVIEW:")
    report.append(f"  Total pickle files: {len(pickle_files)}")
    report.append(f"  Total samples: {len(all_configs)}")
    if len(all_configs) > 0:
        report.append(f"  Parameters per sample: {len(all_configs[0])}")
    if len(all_line_ews) > 0:
        line_ews_array = np.array(all_line_ews)
        report.append(f"  Lines per sample: {line_ews_array.shape[1] if line_ews_array.ndim > 1 else 1}")
    report.append("")
    
    # Eye width statistics
    if len(all_line_ews) > 0:
        line_ews_array = np.array(all_line_ews)
        report.append("EYE WIDTH STATISTICS:")
        report.append(f"  Mean: {line_ews_array.mean():.3f}")
        report.append(f"  Std: {line_ews_array.std():.3f}")
        report.append(f"  Min: {line_ews_array.min():.3f}")
        report.append(f"  Max: {line_ews_array.max():.3f}")
        report.append(f"  Closed eyes: {(line_ews_array < 0).sum()} ({(line_ews_array < 0).mean()*100:.1f}%)")
        report.append("")
    
    # File statistics
    if len(file_stats) > 0:
        summary_df = pd.DataFrame(file_stats)
        report.append("FILE STATISTICS:")
        report.append(f"  Average samples per file: {summary_df['samples'].mean():.1f}")
        report.append(f"  Min samples per file: {summary_df['samples'].min()}")
        report.append(f"  Max samples per file: {summary_df['samples'].max()}")
        report.append("")
    
    # Validation statistics
    if len(comparison_data.get('differences', [])) > 0:
        differences = np.array(comparison_data['differences'])
        rel_errors = np.array(comparison_data['relative_errors'])
        report.append("VALIDATION RESULTS (Pickle vs Simulated):")
        report.append(f"  Samples compared: {len(differences)}")
        report.append(f"  Mean absolute difference: {np.abs(differences).mean():.6f}")
        report.append(f"  Max absolute difference: {np.abs(differences).max():.6f}")
        report.append(f"  RMS difference: {np.sqrt((differences**2).mean()):.6f}")
        report.append(f"  Mean relative error: {rel_errors.mean():.6f}")
        report.append(f"  Max relative error: {rel_errors.max():.6f}")
        report.append(f"  Perfect matches: {(differences == 0).sum()}")
        report.append(f"  Large differences (>1e-3): {(np.abs(differences) > 1e-3).sum()}")
        report.append(f"  Large relative errors (>1%): {(rel_errors > 0.01).sum()}")
        
        # Add interpretation
        if np.abs(differences).max() < 1e-6:
            report.append("  INTERPRETATION: Excellent agreement - differences within numerical precision")
        elif np.abs(differences).max() < 1e-3:
            report.append("  INTERPRETATION: Very good agreement - small numerical differences")
        elif np.abs(differences).max() < 1e-1:
            report.append("  INTERPRETATION: Good agreement - some differences may need investigation")
        else:
            report.append("  INTERPRETATION: Significant differences detected - investigate further")
        report.append("")
    
    # Detailed validation results for each sample
    if detailed_validation_results:
        report.append("DETAILED VALIDATION RESULTS BY SAMPLE:")
        report.append("=" * 50)
        
        for i, result in enumerate(detailed_validation_results):
            report.append(f"\nSample {i+1}: {result['file_name']} (Index: {result['sample_index']})")
            report.append("-" * 60)
            
            # Configuration parameters
            report.append("Configuration:")
            if 'error' in result['config']:
                report.append(f"  ERROR: {result['config']['error']}")
            else:
                config = result['config']
                # Group config parameters for better readability
                for key, value in config.items():
                    if isinstance(value, float):
                        report.append(f"  {key}: {value:.6f}")
                    elif isinstance(value, int):
                        report.append(f"  {key}: {value}")
                    else:
                        report.append(f"  {key}: {str(value)}")
            
            # SNP files and directions
            report.append(f"SNP TX: {result['snp_drv']}")
            report.append(f"SNP RX: {result['snp_odt']}")
            if result['directions']:
                directions_str = str(result['directions'])
                if len(directions_str) > 100:
                    directions_str = directions_str[:100] + "..."
                report.append(f"Directions: {directions_str}")
            
            # Validation results
            if 'error' in result:
                report.append(f"VALIDATION ERROR: {result['error']}")
            else:
                pickle_ew = result['pickle_ew']
                simulated_ew = result['simulated_ew']
                
                # Handle both scalar and array eye widths
                if isinstance(pickle_ew, list) and len(pickle_ew) == 1:
                    pickle_ew = pickle_ew[0]
                    simulated_ew = simulated_ew[0]
                    report.append(f"Pickle EW: {pickle_ew:.6f}")
                    report.append(f"Simulated EW:  {simulated_ew:.6f}")
                    report.append(f"Difference: {result['differences'][0]:.6f}")
                    report.append(f"Rel Error: {result['relative_errors'][0]:.6f}")
                else:
                    # Multiple eye widths (e.g., multiple lines)
                    report.append(f"Pickle EW: {[f'{x:.6f}' for x in pickle_ew]}")
                    report.append(f"Simulated EW:  {[f'{x:.6f}' for x in simulated_ew]}")
                    report.append(f"Differences: {[f'{x:.6f}' for x in result['differences']]}")
                    report.append(f"Rel Errors: {[f'{x:.6f}' for x in result['relative_errors']]}")
                
                report.append(f"Max Abs Diff: {result['max_abs_diff']:.6f}")
                report.append(f"Max Rel Error: {result['max_rel_error']:.6f}")
                report.append(f"Perfect Match: {result['perfect_match']}")
            
            report.append("")  # Add spacing between samples
        
        report.append("=" * 50)
        report.append("")
    
    # Print and save report
    report_text = "\n".join(report)
    print(report_text)
    
    report_file = Path("training_data_summary.txt")
    with open(report_file, 'w') as f:
        f.write(report_text)
    
    # Save detailed validation results as JSON
    if detailed_validation_results:
        json_file = Path("detailed_validation_results.json")
        with open(json_file, 'w') as f:
            json.dump(detailed_validation_results, f, indent=2, default=str)
        print(f"Detailed validation results saved to: {json_file}")
        
    print(f"Report saved to: {report_file}")
    if len(comparison_data.get('differences', [])) > 0:
        print(f"Validation summary saved to: validation_comparison_details.csv")
    
    print("\nAnalysis completed successfully!")

if __name__ == "__main__":
    main() 