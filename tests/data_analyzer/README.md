# Eye Diagram Data Analyzer

This directory contains a comprehensive tool for analyzing, cleaning, and validating the pickle files generated by the data collector.

## Main Script

The primary entry point for this tool is `tests/data_analyzer/main.py`.

## Usage

The tool is structured with subcommands to perform different tasks. The basic structure of a command is:

```bash
python -m tests.data_analyzer.main {analyze,clean,validate} <path_to_pickle_dir> [options]
```

### Commands

There are three main commands available: `analyze`, `clean`, and `validate`.

**Important Note:** The `clean` command removes problematic **records/samples** from pickle files (keeping the file and valid samples), rather than deleting entire files. This preserves valuable simulation results while cleaning out contaminated or duplicate data.

---

### 1. `analyze`

This command performs a full analysis of the data in the specified directory. It provides summary statistics, generates distribution plots, and creates a detailed report.

**Usage:**
```bash
python -m tests.data_analyzer.main analyze /path/to/your/pickle_data
```

**What it does:**
-   Loads all pickle files and aggregates the data.
-   Calculates statistics for eye-widths, direction block sizes, and file sample counts.
-   **WARNING: Analyzes contaminated configurations** - detects samples where config values are parameter names (strings) instead of numeric values.
-   **Analyzes duplicate configurations** within each file and across all files.
-   **Analyzes out-of-range boundary parameters** - detects samples where config values fall outside the valid ranges defined by their param_types (automatically extracted from each file).
-   Generates and saves plots for eye-width distributions (`eye_width_distributions.png`).
-   Generates and saves a detailed text summary (`training_data_summary.txt`) including contamination, duplication, and out-of-range analysis.
-   All outputs are saved to a timestamped directory inside `tests/`, for example `tests/analyzer_output_20231027_123456/`.

---

### 2. `clean`

This command performs in-place cleaning of the pickle files by **removing problematic samples/records** while preserving the files and their valid data. A backup of each modified file is created with a `.bak` extension.

**What gets cleaned by default:**
- ✅ **Contaminated samples** (where config values are strings instead of numbers) - automatically removed
- ✅ **Out-of-range boundary parameters** (where config values fall outside the valid param set ranges) - automatically removed
- ✅ **Duplicate configurations** (optional with `--remove-duplicates`)
- ✅ **Invalid block size patterns** (optional with `--block_size` or `--remove-block-size-1`)

**Note:** Parameter set validation is automatic - the tool reads `param_types` from each pickle file and validates against the corresponding parameter sets defined in `PARAM_SETS_MAP`.

**Usage:**
```bash
python -m tests.data_analyzer.main clean /path/to/your/pickle_data [options]
```

**Options:**

-   `--block_size <N>`:
    -   Only keeps samples where the estimated direction block size is exactly `<N>`.
    -   **Example:** To keep only samples with a block size of 2:
        ```bash
        python -m tests.data_analyzer.main clean ./data --block_size 2
        ```

-   `--remove-block-size-1`:
    -   Removes samples that are known to be "contaminated" from older collection processes, which are identified by having a direction block size of 1.
    -   **Example:**
        ```bash
        python -m tests.data_analyzer.main clean ./data --remove-block-size-1
        ```

-   `--remove-duplicates`:
    -   Removes samples with duplicate configuration values, keeping only the first occurrence of each unique configuration.
    -   **Example:**
        ```bash
        python -m tests.data_analyzer.main clean ./data --remove-duplicates
        ```

-   `--keep-contaminated`:
    -   **By default, contaminated samples (where config values are strings) are automatically removed.**
    -   Use this flag to keep contaminated samples if you want to inspect them manually.
    -   **Example:**
        ```bash
        python -m tests.data_analyzer.main clean ./data --keep-contaminated
        ```

-   `--remove-legacy`:
    -   Removes samples that use legacy parameter naming conventions (e.g., `R_tx`, `R_rx`, `C_tx`, `C_rx`, `L_tx`, `L_rx`).
    -   Modern samples use the updated naming (e.g., `R_drv`, `R_odt`, `C_drv`, `C_odt`, `L_drv`, `L_odt`).
    -   **Example:**
        ```bash
        python -m tests.data_analyzer.main clean ./data --remove-legacy
        ```

**Combined Example:**
To clean all issues at once (remove contaminated samples, duplicates, legacy format, and block size 1 patterns):
```bash
python -m tests.data_analyzer.main clean ./data --remove-block-size-1 --remove-duplicates --remove-legacy
```

**Important Notes:**
- This command **does NOT delete files** - it only removes bad records/samples from within files
- Valid samples remain in the file, preserving your valuable simulation results
- A `.bak` backup is created for each modified file
- The command provides a summary showing how many samples were removed

---

### 3. `validate`

This command validates the eye-width data in the pickle files by re-running the simulation for a subset of samples and comparing the results. This is useful for verifying data integrity and the consistency of the simulation engine.

**Usage:**
```bash
python -m tests.data_analyzer.main validate /path/to/your/pickle_data [options]
```

**Options:**

-   `--max_files <N>`:
    -   The maximum number of randomly selected pickle files to validate. Defaults to `3`.
    -   **Example:** To validate 10 files:
        ```bash
        python -m tests.data_analyzer.main validate ./data --max_files 10
        ```

-   `--max_samples <M>`:
    -   The maximum number of samples to validate from *each* selected file. Defaults to `5`.
    -   **Example:** To validate 20 samples from each file:
        ```bash
        python -m tests.data_analyzer.main validate ./data --max_samples 20
        ```

**What it does:**
-   Randomly selects files and samples to validate.
-   For each sample, it reconstructs the configuration and re-runs the simulation.
-   It calculates the difference and relative error between the stored eye-width and the newly simulated one.
-   Generates a detailed CSV report (`validation_comparison_details.csv`) and plots comparing the results.
-   All outputs are saved to a timestamped directory inside `tests/`.
