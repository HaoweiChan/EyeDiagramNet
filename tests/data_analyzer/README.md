# Eye Diagram Data Analyzer

This directory contains a comprehensive tool for analyzing, cleaning, and validating the pickle files generated by the data collector.

## Main Script

The primary entry point for this tool is `tests/data_analyzer/main.py`.

## Usage

The tool is structured with subcommands to perform different tasks. The basic structure of a command is:

```bash
python -m tests.data_analyzer.main {analyze,clean,validate} <path_to_pickle_dir> [options]
```

### Commands

There are three main commands available: `analyze`, `clean`, and `validate`.

---

### 1. `analyze`

This command performs a full analysis of the data in the specified directory. It provides summary statistics, generates distribution plots, and creates a detailed report.

**Usage:**
```bash
python -m tests.data_analyzer.main analyze /path/to/your/pickle_data
```

**What it does:**
-   Loads all pickle files and aggregates the data.
-   Calculates statistics for eye-widths, direction block sizes, and file sample counts.
-   **WARNING: Analyzes contaminated configurations** - detects samples where config values are parameter names (strings) instead of numeric values.
-   **Analyzes duplicate configurations** within each file and across all files.
-   Generates and saves plots for eye-width distributions (`eye_width_distributions.png`).
-   Generates and saves a detailed text summary (`training_data_summary.txt`) including contamination and duplication analysis.
-   All outputs are saved to a timestamped directory inside `tests/`, for example `tests/analyzer_output_20231027_123456/`.

---

### 2. `clean`

This command performs in-place cleaning of the pickle files. It can remove samples based on several criteria. A backup of each modified file is created with a `.bak` extension.

**Usage:**
```bash
python -m tests.data_analyzer.main clean /path/to/your/pickle_data [options]
```

**Options:**

-   `--block_size <N>`:
    -   Only keeps samples where the estimated direction block size is exactly `<N>`.
    -   **Example:** To keep only samples with a block size of 2:
        ```bash
        python -m tests.data_analyzer.main clean ./data --block_size 2
        ```

-   `--remove-block-size-1`:
    -   Removes samples that are known to be "contaminated" from older collection processes, which are identified by having a direction block size of 1.
    -   **Example:**
        ```bash
        python -m tests.data_analyzer.main clean ./data --remove-block-size-1
        ```

-   `--remove-duplicates`:
    -   Removes samples with duplicate configuration values, keeping only the first occurrence of each unique configuration.
    -   **Example:**
        ```bash
        python -m tests.data_analyzer.main clean ./data --remove-duplicates
        ```

-   `--keep-contaminated`:
    -   **By default, contaminated samples (where config values are strings) are removed.**
    -   Use this flag to keep contaminated samples if you want to inspect them manually.
    -   **Example:**
        ```bash
        python -m tests.data_analyzer.main clean ./data --keep-contaminated
        ```

You can combine these options. For example, to only keep samples with block size 2, remove block size 1 patterns, remove duplicates, and remove contaminated samples:
```bash
python -m tests.data_analyzer.main clean ./data --block_size 2 --remove-block-size-1 --remove-duplicates
```

---

### 3. `delete-contaminated`

This command **deletes entire pickle files** that contain contaminated config data (where config values are parameter names instead of numeric values). This is useful for removing files that were generated with the buggy `to_list()` return order.

**WARNING:** This command permanently deletes files. Always run with `--dry-run` first to see what would be deleted!

**Usage (Dry Run - Safe):**
```bash
python -m tests.data_analyzer.main delete-contaminated /path/to/your/pickle_data
```
or
```bash
python -m tests.data_analyzer.main delete-contaminated /path/to/your/pickle_data --dry-run
```

**Usage (Actually Delete Files):**
```bash
python -m tests.data_analyzer.main delete-contaminated /path/to/your/pickle_data --force
```

**What it does:**
-   Scans all pickle files for contaminated config data.
-   A file is considered contaminated if **ANY** sample has config values that are strings.
-   In dry-run mode (default), only reports what would be deleted without actually deleting.
-   With `--force` flag, permanently deletes all contaminated files.
-   Provides detailed statistics about contamination and deletion.

---

### 4. `validate`

This command validates the eye-width data in the pickle files by re-running the simulation for a subset of samples and comparing the results. This is useful for verifying data integrity and the consistency of the simulation engine.

**Usage:**
```bash
python -m tests.data_analyzer.main validate /path/to/your/pickle_data [options]
```

**Options:**

-   `--max_files <N>`:
    -   The maximum number of randomly selected pickle files to validate. Defaults to `3`.
    -   **Example:** To validate 10 files:
        ```bash
        python -m tests.data_analyzer.main validate ./data --max_files 10
        ```

-   `--max_samples <M>`:
    -   The maximum number of samples to validate from *each* selected file. Defaults to `5`.
    -   **Example:** To validate 20 samples from each file:
        ```bash
        python -m tests.data_analyzer.main validate ./data --max_samples 20
        ```

**What it does:**
-   Randomly selects files and samples to validate.
-   For each sample, it reconstructs the configuration and re-runs the simulation.
-   It calculates the difference and relative error between the stored eye-width and the newly simulated one.
-   Generates a detailed CSV report (`validation_comparison_details.csv`) and plots comparing the results.
-   All outputs are saved to a timestamped directory inside `tests/`.
